{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision\n",
        "Trees, SVM, and Naive Bayes|\n",
        "Assignment"
      ],
      "metadata": {
        "id": "Uap9gUGl2k4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "  - Information Gain is a metric used in decision trees to determine the best attribute for splitting data at each node. It quantifies the reduction in entropy (uncertainty) about the target variable after a split. By calculating Information Gain for each attribute, the algorithm selects the one that provides the most significant reduction in entropy. This process continues recursively, building a tree that efficiently classifies or predicts the target variable.\n",
        "\n",
        "  Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "  -   Gini Impurity:\n",
        "      -   It Measures the probability of misclassifying a randomly chosen element.\n",
        "      -    Calculated as: 1 - Σ p(i)\\^2.\n",
        "      - Computationally less expensive due to the absence of logarithms.\n",
        "      -   Tends to favor splits with distinct classes.\n",
        "-   Gini Entropy:\n",
        "      -   It Measures the average information needed to identify the class.\n",
        "      -   Calculated as: - Σ p(i) \\* log2(p(i)).\n",
        "      -   Slightly more sensitive to changes in class probabilities.\n",
        "      -   Tends to favor splits that result in more balanced branches.\n",
        "-   Use Cases:\n",
        "    -   Gini Impurity is often preferred for its efficiency.\n",
        "    -   Entropy may be chosen for a more balanced tree structure.\n",
        "\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "  - Pre-pruning, also known as early stopping, is a method used in decision trees to prevent overfitting. It involves setting criteria to stop the growth of the tree before it fully classifies the training data. This can include limiting the tree's depth, the number of samples in a leaf node, or the minimum impurity decrease required for a split. The goal is to create a simpler tree that generalizes better to unseen data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nIcci3WNxM3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "bgS5IlHZz5Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OWZISiBwx1j",
        "outputId": "ecb9d698-e7b1-4dcb-f451-7bcec4503984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.01911001911001911\n",
            "petal length (cm): 0.8932635518001373\n",
            "petal width (cm): 0.08762642908984374\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "dtree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(dtree.feature_importances_):\n",
        "    print(f\"{iris.feature_names[i]}: {importance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning model used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes with the widest possible margin. SVMs are effective in high-dimensional spaces and can use kernel functions to handle non-linearly separable data.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "  - The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map data into a higher-dimensional space without explicitly calculating the transformation. This allows SVMs to perform non-linear classification by using kernel functions, such as the radial basis function (RBF) or polynomial kernels, which compute the dot product of data points in the transformed space.\n",
        "\n"
      ],
      "metadata": {
        "id": "yc8O7CRB0Cci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset."
      ],
      "metadata": {
        "id": "kXkNY6Mj0ibY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_svm_predictions = linear_svm.predict(X_test)\n",
        "linear_svm_accuracy = accuracy_score(y_test, linear_svm_predictions)\n",
        "\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_svm_predictions = rbf_svm.predict(X_test)\n",
        "rbf_svm_accuracy = accuracy_score(y_test, rbf_svm_predictions)\n",
        "\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_svm_accuracy)\n",
        "print(\"RBF SVM Accuracy:\", rbf_svm_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDYCGKuBzxz0",
        "outputId": "9099b372-aaac-41cf-e1ee-0f933a48c4a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "  - The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem, used for classification tasks. It's called \"naive\" because it assumes that all features in the dataset are conditionally independent of each other given the class label. This means it assumes that the presence or absence of a particular feature does not affect the presence or absence of any other feature. This assumption simplifies the calculations but is often unrealistic in real-world scenarios. Despite this strong assumption, Naive Bayes classifiers often perform surprisingly well, especially in text classification and spam filtering.\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        " -   Gaussian Naive Bayes: This variant assumes that the features follow a Gaussian (normal) distribution. It's typically used when the features are continuous. The classifier calculates the mean and standard deviation of each feature for each class and uses these parameters to estimate the probability of a data point belonging to a particular class.\n",
        "\n",
        "-   Multinomial Naive Bayes: This is commonly used for text classification. It assumes that the features represent the frequency of words in a document. The features are counts (e.g., term frequency), and the model calculates the probability of each word given a class, using these counts. It's suitable for discrete data, especially for text where the frequency of words matters.\n",
        "\n",
        "-   Bernoulli Naive Bayes: This variant is used when features are binary (0 or 1). It's suitable for data where the presence or absence of a feature is important. It models the probability of each feature being present (1) or absent (0) given the class. It's often used for text classification with binary features indicating the presence or absence of words.\n"
      ],
      "metadata": {
        "id": "s1EW3qXL1GHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer.data, cancer.target, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeSNtYAg07ls",
        "outputId": "6b8bdf83-c85c-4920-85d6-8cb69ce822f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}